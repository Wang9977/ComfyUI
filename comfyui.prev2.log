## ComfyUI-Manager: installing dependencies done.
[2024-06-17 18:02] ** ComfyUI startup time: 2024-06-17 18:02:52.787087
[2024-06-17 18:02] ** Platform: Darwin
[2024-06-17 18:02] ** Python version: 3.8.8 (default, Apr 13 2021, 12:59:45) 
[Clang 10.0.0 ]
[2024-06-17 18:02] ** Python executable: /Users/wangyuan198/Downloads/anaconda3/bin/python
[2024-06-17 18:02] ** Log path: /Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/comfyui.log
[2024-06-17 18:02] 
Prestartup times for custom nodes:
[2024-06-17 18:02]    1.3 seconds: /Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/custom_nodes/ComfyUI-Manager
[2024-06-17 18:02] 
Total VRAM 16384 MB, total RAM 16384 MB
[2024-06-17 18:02] pytorch version: 2.2.2
[2024-06-17 18:02] Forcing FP16.
[2024-06-17 18:02] Set vram state to: SHARED
[2024-06-17 18:02] Device: mps
[2024-06-17 18:02] Using sub quadratic optimization for cross attention, if you have memory or speed issues try using: --use-split-cross-attention
[2024-06-17 18:02] ### Loading: ComfyUI-Manager (V2.38.1)
[2024-06-17 18:02] ### ComfyUI Revision: 2261 [6425252c] | Released on '2024-06-16'
[2024-06-17 18:02] Traceback (most recent call last):
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/nodes.py", line 1906, in load_custom_node
    module_spec.loader.exec_module(module)
  File "<frozen importlib._bootstrap_external>", line 779, in exec_module
  File "<frozen importlib._bootstrap_external>", line 915, in get_code
  File "<frozen importlib._bootstrap_external>", line 972, in get_data
FileNotFoundError: [Errno 2] No such file or directory: '/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/custom_nodes/.ipynb_checkpoints/__init__.py'

[2024-06-17 18:02] Cannot import /Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/custom_nodes/.ipynb_checkpoints module for custom nodes: [Errno 2] No such file or directory: '/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/custom_nodes/.ipynb_checkpoints/__init__.py'
[2024-06-17 18:02] 
Import times for custom nodes:
[2024-06-17 18:02]    0.0 seconds: /Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/custom_nodes/websocket_image_save.py
[2024-06-17 18:02]    0.0 seconds: /Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/custom_nodes/AIGODLIKE-COMFYUI-TRANSLATION
[2024-06-17 18:02]    0.0 seconds (IMPORT FAILED): /Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/custom_nodes/.ipynb_checkpoints
[2024-06-17 18:02]    0.1 seconds: /Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/custom_nodes/ComfyUI-Manager
[2024-06-17 18:02] 
[2024-06-17 18:02] Starting server

[2024-06-17 18:02] To see the GUI go to: http://127.0.0.1:8188
[2024-06-17 18:02] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json
[2024-06-17 18:02] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/github-stats.json
[2024-06-17 18:02] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json
[2024-06-17 18:02] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json
[2024-06-17 18:02] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json
[2024-06-17 18:03] got prompt
[2024-06-17 18:03] model_type EPS
[2024-06-17 18:03] Using split attention in VAE
[2024-06-17 18:03] Using split attention in VAE
[2024-06-17 18:03] Requested to load SDXLClipModel
[2024-06-17 18:03] Loading 1 new model
[2024-06-17 18:03] Requested to load AutoencoderKL
[2024-06-17 18:03] Loading 1 new model
[2024-06-17 18:03] Requested to load SDXL
[2024-06-17 18:03] Loading 1 new model
[2024-06-17 18:07] 100%|██████████████████████████████████████████████████████████████████████████████████████████| 20/20 [03:23<00:00,  9.91s/it]100%|██████████████████████████████████████████████████████████████████████████████████████████| 20/20 [03:23<00:00, 10.17s/it]
[2024-06-17 18:07] Requested to load AutoencoderKL
[2024-06-17 18:07] Loading 1 new model
[2024-06-17 18:07] !!! Exception during processing!!! MPS backend out of memory (MPS allocated: 5.75 GB, other allocations: 797.23 MB, max allowed: 6.77 GB). Tried to allocate 288.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
[2024-06-17 18:07] Traceback (most recent call last):
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/execution.py", line 151, in recursive_execute
    output_data, output_ui = get_output_data(obj, input_data_all)
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/execution.py", line 81, in get_output_data
    return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/execution.py", line 74, in map_node_over_list
    results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/nodes.py", line 268, in decode
    return (vae.decode(samples["samples"]), )
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/comfy/sd.py", line 325, in decode
    pixel_samples[x:x+batch_number] = self.process_output(self.first_stage_model.decode(samples).to(self.output_device).float())
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/comfy/ldm/models/autoencoder.py", line 200, in decode
    dec = self.decoder(dec, **decoder_kwargs)
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/comfy/ldm/modules/diffusionmodules/model.py", line 635, in forward
    h = self.up[i_level].block[i_block](h, temb, **kwargs)
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/comfy/ldm/modules/diffusionmodules/model.py", line 140, in forward
    h = self.norm1(h)
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/wangyuan198/Documents/study/ml-in-fe/ComfyUI/comfy/ops.py", line 108, in forward
    return super().forward(*args, **kwargs)
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 287, in forward
    return F.group_norm(
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 2561, in group_norm
    return torch.group_norm(input, num_groups, weight, bias, eps, torch.backends.cudnn.enabled)
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/_refs/__init__.py", line 3056, in native_group_norm
    out, mean, rstd = _normalize(input_reshaped, reduction_dims, eps)
  File "/Users/wangyuan198/Downloads/anaconda3/lib/python3.8/site-packages/torch/_refs/__init__.py", line 3018, in _normalize
    out = (a - mean) * rstd
RuntimeError: MPS backend out of memory (MPS allocated: 5.75 GB, other allocations: 797.23 MB, max allowed: 6.77 GB). Tried to allocate 288.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).

[2024-06-17 18:07] Prompt executed in 247.60 seconds
[2024-06-17 18:08] got prompt
[2024-06-17 18:09] got prompt
[2024-06-17 18:09] got prompt
[2024-06-17 18:09] 
Stopped server
